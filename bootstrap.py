# bootstrap.py â€” Import existing GEO corpus into GEO Pulse
# Reads SignalSynth-GEO data and maps it to GEO Pulse format.
import json
import os
from datetime import datetime

SOURCE_DIR = "/Users/garrettdallas/signalsynth-geo/data"
TARGET_DIR = "data"


def run_bootstrap():
    """Import SignalSynth-GEO corpus into GEO Pulse data format."""
    os.makedirs(TARGET_DIR, exist_ok=True)

    total_imported = 0

    # 1. Import Reddit posts
    reddit_src = os.path.join(SOURCE_DIR, "all_scraped_posts.json")
    if os.path.exists(reddit_src):
        with open(reddit_src, "r", encoding="utf-8") as f:
            posts = json.load(f)

        # Map to GEO Pulse format
        mapped = []
        for p in posts:
            mapped.append({
                "text": p.get("text", ""),
                "title": p.get("title", ""),
                "source": p.get("source", "Reddit"),
                "url": p.get("url", ""),
                "username": p.get("username", ""),
                "post_date": p.get("post_date", ""),
                "_logged_date": p.get("_logged_date", datetime.now().isoformat()),
                "search_term": p.get("search_term", ""),
                "score": p.get("score", 0),
                "num_comments": p.get("num_comments", 0),
                "post_id": p.get("post_id", ""),
                "subreddit": p.get("subreddit", ""),
            })

        out_path = os.path.join(TARGET_DIR, "scraped_reddit.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(mapped, f, ensure_ascii=False, indent=2)
        print(f"  Reddit: {len(mapped)} posts -> {out_path}")
        total_imported += len(mapped)

    # 2. Import competitor posts (also Reddit-sourced)
    comp_src = os.path.join(SOURCE_DIR, "scraped_competitor_posts.json")
    if os.path.exists(comp_src):
        with open(comp_src, "r", encoding="utf-8") as f:
            comp_posts = json.load(f)

        # Append to Reddit data (dedup by post_id)
        existing_ids = set()
        if os.path.exists(os.path.join(TARGET_DIR, "scraped_reddit.json")):
            with open(os.path.join(TARGET_DIR, "scraped_reddit.json"), "r") as f:
                existing = json.load(f)
            existing_ids = {p.get("post_id") for p in existing}

            new_comp = []
            for p in comp_posts:
                pid = p.get("post_id", "")
                if pid and pid not in existing_ids:
                    new_comp.append({
                        "text": p.get("text", ""),
                        "title": p.get("title", ""),
                        "source": p.get("source", "Reddit"),
                        "url": p.get("url", ""),
                        "username": p.get("username", ""),
                        "post_date": p.get("post_date", ""),
                        "_logged_date": p.get("_logged_date", datetime.now().isoformat()),
                        "search_term": p.get("search_term", ""),
                        "score": p.get("score", 0),
                        "num_comments": p.get("num_comments", 0),
                        "post_id": pid,
                        "subreddit": p.get("subreddit", ""),
                    })

            existing.extend(new_comp)
            with open(os.path.join(TARGET_DIR, "scraped_reddit.json"), "w", encoding="utf-8") as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)
            print(f"  Competitor posts: {len(new_comp)} new -> merged into scraped_reddit.json")
            total_imported += len(new_comp)

    # 3. Import discovered sources
    disc_src = os.path.join(SOURCE_DIR, "discovered_sources.json")
    if os.path.exists(disc_src):
        with open(disc_src, "r", encoding="utf-8") as f:
            sources = json.load(f)

        out_path = os.path.join(TARGET_DIR, "discovered_sources.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(sources, f, ensure_ascii=False, indent=2)
        print(f"  Discovered sources: {len(sources)} -> {out_path}")

    # 4. Import trend data if available
    trend_src = os.path.join(SOURCE_DIR, "trend_data.json")
    if os.path.exists(trend_src):
        with open(trend_src, "r", encoding="utf-8") as f:
            trends = json.load(f)

        # Save as bootstrap reference (will be regenerated by pipeline)
        out_path = os.path.join(TARGET_DIR, "bootstrap_trends.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(trends, f, ensure_ascii=False, indent=2)
        print(f"  Trend data: bootstrapped -> {out_path}")

    # 5. Create empty placeholder files for other scrapers
    for name in ["scraped_hackernews.json", "scraped_slack.json",
                  "scraped_producthunt.json", "scraped_news_rss.json",
                  "scraped_g2.json"]:
        path = os.path.join(TARGET_DIR, name)
        if not os.path.exists(path):
            with open(path, "w") as f:
                json.dump([], f)

    print(f"\n  Bootstrap complete: {total_imported} total posts imported")
    return total_imported


if __name__ == "__main__":
    run_bootstrap()
